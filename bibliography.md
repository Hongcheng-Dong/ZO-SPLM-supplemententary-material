# Bibliography and References

## Primary References

1. **Nesterov, Y., & Spokoiny, V.** (2017). Random gradient-free minimization of convex functions. *Foundations of Computational Mathematics*, 17(2), 527-566.

2. **Chen, P. Y., Zhang, H., Sharma, Y., Yi, J., & Hsieh, C. J.** (2017). ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. *Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security*, 15-26.

3. **Liu, S., Kailkhura, B., Chen, P. Y., Ting, P., Chang, S., & Amini, L.** (2018). Zeroth-order stochastic variance reduction for nonconvex optimization. *Advances in Neural Information Processing Systems*, 31.

4. **Duchi, J. C., Jordan, M. I., Wainwright, M. J., & Wibisono, A.** (2015). Optimal rates for zero-order convex optimization: The power of two function evaluations. *IEEE Transactions on Information Theory*, 61(5), 2788-2806.

## Theoretical Foundations

5. **Polyak, B. T.** (1987). *Introduction to optimization*. Optimization Software.

6. **Nemirovskii, A., & Yudin, D.** (1983). *Problem complexity and method efficiency in optimization*. Wiley-Interscience.

7. **Bubeck, S.** (2015). Convex optimization: Algorithms and complexity. *Foundations and Trends in Machine Learning*, 8(3-4), 231-357.

8. **Beck, A.** (2017). *First-order methods in optimization*. SIAM.

## Zeroth-Order Methods

9. **Spall, J. C.** (2003). *Introduction to stochastic search and optimization: estimation, simulation, and control*. John Wiley & Sons.

10. **Conn, A. R., Scheinberg, K., & Vicente, L. N.** (2009). *Introduction to derivative-free optimization*. SIAM.

11. **Larson, J., Menickelly, M., & Wild, S. M.** (2019). Derivative-free optimization methods. *Acta Numerica*, 28, 287-404.

12. **Shamir, O.** (2017). An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. *Journal of Machine Learning Research*, 18(1), 1703-1713.

## Stochastic Optimization

13. **Robbins, H., & Monro, S.** (1951). A stochastic approximation method. *The Annals of Mathematical Statistics*, 22(3), 400-407.

14. **Polyak, B. T., & Juditsky, A. B.** (1992). Acceleration of stochastic approximation by averaging. *SIAM Journal on Control and Optimization*, 30(4), 838-855.

15. **Nemirovski, A., Juditsky, A., Lan, G., & Shapiro, A.** (2009). Robust stochastic approximation approach to stochastic programming. *SIAM Journal on Optimization*, 19(4), 1574-1609.

## Convergence Analysis

16. **Bottou, L., Curtis, F. E., & Nocedal, J.** (2018). Optimization methods for large-scale machine learning. *SIAM Review*, 60(2), 223-311.

17. **Ghadimi, S., & Lan, G.** (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. *SIAM Journal on Optimization*, 23(4), 2341-2368.

18. **Allen-Zhu, Z.** (2017). Natasha: Faster non-convex stochastic optimization via strongly non-convex parameter. *Proceedings of the 34th International Conference on Machine Learning*, 89-97.

## Finite Difference Methods

19. **Kiefer, J., & Wolfowitz, J.** (1952). Stochastic estimation of the maximum of a regression function. *The Annals of Mathematical Statistics*, 23(3), 462-466.

20. **Spall, J. C.** (1992). Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. *IEEE Transactions on Automatic Control*, 37(3), 332-341.

21. **Flaxman, A. D., Kalai, A. T., & McMahan, H. B.** (2005). Online convex optimization in the bandit setting: gradient descent without a gradient. *Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms*, 385-394.

## Concentration Inequalities

22. **Boucheron, S., Lugosi, G., & Massart, P.** (2013). *Concentration inequalities: A nonasymptotic theory of independence*. Oxford University Press.

23. **McDiarmid, C.** (1989). On the method of bounded differences. *Surveys in Combinatorics*, 141, 148-188.

24. **Azuma, K.** (1967). Weighted sums of certain dependent random variables. *Tohoku Mathematical Journal*, 19(3), 357-367.

## Applications

25. **Ilyas, A., Engstrom, L., Athalye, A., & Lin, J.** (2018). Black-box adversarial attacks with limited queries and information. *Proceedings of the 35th International Conference on Machine Learning*, 2137-2146.

26. **Tu, C. C., Ting, P., Chen, P. Y., Liu, S., Zhang, H., Yi, J., ... & Wang, S.** (2019). AutoZOOM: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. *Proceedings of the AAAI Conference on Artificial Intelligence*, 33, 742-749.

27. **Chen, J., Jordan, M. I., & Wainwright, M. J.** (2019). HopSkipJumpAttack: A query-efficient decision-based attack. *IEEE Symposium on Security and Privacy*, 1277-1294.

## Complexity Theory

28. **Nemirovsky, A. S., & Yudin, D. B.** (1983). *Problem complexity and method efficiency in optimization*. John Wiley & Sons.

29. **Jamieson, K. G., Nowak, R., & Recht, B.** (2012). Query complexity of derivative-free optimization. *Advances in Neural Information Processing Systems*, 25.

30. **Shamir, O.** (2013). On the complexity of bandit and derivative-free stochastic convex optimization. *Proceedings of the 26th Annual Conference on Learning Theory*, 3-24.

## Recent Advances

31. **Gasnikov, A., Dvurechensky, P., Gorbunov, E., Vorontsova, E., Selikhanovych, D., & Uribe, C. A.** (2022). The power of first-order smooth optimization for black-box non-smooth problems. *International Conference on Machine Learning*, 7241-7265.

32. **Krishnamurthy, A., Agarwal, A., & Langford, J.** (2019). Efficient algorithms for smooth minimax optimization. *Advances in Neural Information Processing Systems*, 32.

33. **Hajek, B., Yang, T., & Zhu, Y.** (2019). Online learning with bandit feedback under heavy tail noise. *Proceedings of the 36th International Conference on Machine Learning*, 2530-2539.

## Software and Implementation

34. **Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, E.** (2011). Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12, 2825-2830.

35. **Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S.** (2019). PyTorch: An imperative style, high-performance deep learning library. *Advances in Neural Information Processing Systems*, 32.

## Mathematical Background

36. **Rudin, W.** (1976). *Principles of mathematical analysis*. McGraw-Hill.

37. **Folland, G. B.** (1999). *Real analysis: modern techniques and their applications*. John Wiley & Sons.

38. **Boyd, S., & Vandenberghe, L.** (2004). *Convex optimization*. Cambridge University Press.

39. **Rockafellar, R. T.** (1970). *Convex analysis*. Princeton University Press.

## Related Survey Papers

40. **Rios, L. M., & Sahinidis, N. V.** (2013). Derivative-free optimization: a review of algorithms and comparison of software implementations. *Journal of Global Optimization*, 56(3), 1247-1293.

41. **Audet, C., & Hare, W.** (2017). *Derivative-free and blackbox optimization*. Springer.

42. **Larson, J., Menickelly, M., & Wild, S. M.** (2019). Derivative-free optimization methods. *Acta Numerica*, 28, 287-404.

---

## Conference Proceedings and Workshops

### ICML (International Conference on Machine Learning)
- Proceedings from 2015-2023 for zeroth-order optimization papers

### NeurIPS (Conference on Neural Information Processing Systems)  
- Proceedings from 2015-2023 for derivative-free optimization papers

### ICLR (International Conference on Learning Representations)
- Proceedings from 2018-2023 for black-box optimization papers

### SIAM Optimization Conferences
- Proceedings from 2017-2023 for theoretical analysis papers

---

## Online Resources

- **arXiv preprints**: https://arxiv.org/list/math.OC/recent
- **Optimization Online**: http://www.optimization-online.org/
- **NEOS Guide**: https://neos-guide.org/optimization-taxonomy
- **COIN-OR**: https://www.coin-or.org/

---

*This bibliography provides comprehensive coverage of the theoretical foundations, algorithmic developments, and practical applications relevant to ZO-SPLM and zeroth-order optimization methods.*